{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural_Regressor(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=6, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=6, out_features=3, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.1, inplace=False)\n",
            "    (6): Linear(in_features=3, out_features=2, bias=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Dropout(p=0.1, inplace=False)\n",
            "    (9): Linear(in_features=2, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 164002.000000  [   20/  445]\n",
            "loss: 92413.546875  [  220/  445]\n",
            "loss: 69140.109375  [  420/  445]\n",
            "Avg loss: 23389.952148 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 82847.265625  [   20/  445]\n",
            "loss: 49819.445312  [  220/  445]\n",
            "loss: 67142.085938  [  420/  445]\n",
            "Avg loss: 53182.546224 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 53948.710938  [   20/  445]\n",
            "loss: 40804.230469  [  220/  445]\n",
            "loss: 52213.746094  [  420/  445]\n",
            "Avg loss: 44456.453776 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 65074.800781  [   20/  445]\n",
            "loss: 53495.039062  [  220/  445]\n",
            "loss: 65794.015625  [  420/  445]\n",
            "Avg loss: 48408.803385 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 59146.769531  [   20/  445]\n",
            "loss: 25181.746094  [  220/  445]\n",
            "loss: 58715.617188  [  420/  445]\n",
            "Avg loss: 27634.410807 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 34376.066406  [   20/  445]\n",
            "loss: 64943.406250  [  220/  445]\n",
            "loss: 64110.351562  [  420/  445]\n",
            "Avg loss: 53088.291667 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 39138.937500  [   20/  445]\n",
            "loss: 47240.738281  [  220/  445]\n",
            "loss: 70943.062500  [  420/  445]\n",
            "Avg loss: 44991.659505 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 64479.519531  [   20/  445]\n",
            "loss: 33961.890625  [  220/  445]\n",
            "loss: 66622.890625  [  420/  445]\n",
            "Avg loss: 54928.591146 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 65855.515625  [   20/  445]\n",
            "loss: 40341.726562  [  220/  445]\n",
            "loss: 61210.023438  [  420/  445]\n",
            "Avg loss: 45241.268880 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 65969.585938  [   20/  445]\n",
            "loss: 23691.865234  [  220/  445]\n",
            "loss: 60973.492188  [  420/  445]\n",
            "Avg loss: 51172.800130 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 63570.320312  [   20/  445]\n",
            "loss: 42467.734375  [  220/  445]\n",
            "loss: 56487.238281  [  420/  445]\n",
            "Avg loss: 41744.660807 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 49710.292969  [   20/  445]\n",
            "loss: 49307.871094  [  220/  445]\n",
            "loss: 71503.890625  [  420/  445]\n",
            "Avg loss: 50222.229818 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 78370.750000  [   20/  445]\n",
            "loss: 41543.132812  [  220/  445]\n",
            "loss: 71810.968750  [  420/  445]\n",
            "Avg loss: 47340.678385 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 35982.164062  [   20/  445]\n",
            "loss: 44272.113281  [  220/  445]\n",
            "loss: 107397.289062  [  420/  445]\n",
            "Avg loss: 49847.713542 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 12951.783203  [   20/  445]\n",
            "loss: 36194.117188  [  220/  445]\n",
            "loss: 43868.593750  [  420/  445]\n",
            "Avg loss: 47145.603516 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 60572.519531  [   20/  445]\n",
            "loss: 65255.804688  [  220/  445]\n",
            "loss: 46559.343750  [  420/  445]\n",
            "Avg loss: 46801.637370 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 33965.750000  [   20/  445]\n",
            "loss: 46041.535156  [  220/  445]\n",
            "loss: 47882.316406  [  420/  445]\n",
            "Avg loss: 47573.528646 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 81964.468750  [   20/  445]\n",
            "loss: 43977.160156  [  220/  445]\n",
            "loss: 58881.582031  [  420/  445]\n",
            "Avg loss: 45870.674479 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 31108.537109  [   20/  445]\n",
            "loss: 34940.832031  [  220/  445]\n",
            "loss: 85703.546875  [  420/  445]\n",
            "Avg loss: 41498.689453 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 70690.484375  [   20/  445]\n",
            "loss: 29891.730469  [  220/  445]\n",
            "loss: 67551.625000  [  420/  445]\n",
            "Avg loss: 47178.643880 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def model_training():\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    data_big = pd.read_csv(\"/Users/ponynie/Developer/Python_Code/HandOnMLChula/Kaggle/df_train.csv\")\n",
        "    data_big = data_big.sample(frac=1).reset_index(drop=True)\n",
        "    data_big_target = data_big['SalePrice']\n",
        "    data_big_features = data_big.drop(['SalePrice'], axis=1)\n",
        "    data_big_features_train, data_big_features_test, data_big_target_train, data_big_target_test = train_test_split(data_big_features, data_big_target, test_size=0.2, random_state=42)\n",
        "    #train80% -> test 20%\n",
        "    total_records = data_big_features_train.shape[0]\n",
        "    #total records is 80% remaining\n",
        "\n",
        "    #*HYPERPARAMETER 1\n",
        "    t = 0.2\n",
        "    \n",
        "    #*LINEAR REGRESSION SPLIT\n",
        "    data_features_linear = data_big_features_train[:int(total_records*0.25)]\n",
        "    data_target_linear = data_big_target_train[:int(total_records*0.25)]   \n",
        "    data_features_linear_train, data_features_linear_test, data_target_linear_train, data_target_linear_test = train_test_split(data_features_linear, data_target_linear, test_size=t, random_state=42) \n",
        "\n",
        "    #*TREE REGRESSION SPLIT\n",
        "    data_features_tree = data_big_features_train[int(total_records*0.25):int(total_records*0.5)]\n",
        "    data_target_tree = data_big_target_train[int(total_records*0.25):int(total_records*0.5)]\n",
        "    data_features_tree_train, data_features_tree_test, data_target_tree_train, data_target_tree_test = train_test_split(data_features_tree, data_target_tree, test_size=t, random_state=42)\n",
        "\n",
        "    #*FOREST REGRESSION SPLIT\n",
        "    data_features_forest = data_big_features_train[int(total_records*0.5):int(total_records*0.75)]\n",
        "    data_target_forest = data_big_target_train[int(total_records*0.5):int(total_records*0.75)]\n",
        "    data_features_forest_train, data_features_forest_test, data_target_forest_train, data_target_forest_test = train_test_split(data_features_forest, data_target_forest, test_size=t, random_state=42)\n",
        "\n",
        "    #*BOOST REGRESSION SPLIT\n",
        "    data_features_boost = data_big_features_train[int(total_records*0.75):]\n",
        "    data_target_boost = data_big_target_train[int(total_records*0.75):]\n",
        "    data_features_boost_train, data_features_boost_test, data_target_boost_train, data_target_boost_test = train_test_split(data_features_boost, data_target_boost, test_size=t, random_state=42)\n",
        "\n",
        "    #*LINEAR REGRESSION MODEL\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    linear_model = LinearRegression()\n",
        "    linear_model.fit(data_features_linear_train, data_target_linear_train)\n",
        "    linear_model.score(data_features_linear_test, data_target_linear_test)\n",
        "    \n",
        "    #*TREE REGRESSION MODEL\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    tree_model = DecisionTreeRegressor()\n",
        "    tree_model.fit(data_features_tree_train, data_target_tree_train)\n",
        "    tree_model.score(data_features_tree_test, data_target_tree_test)\n",
        "\n",
        "    #*FOREST REGRESSION MODEL\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    forest_model = RandomForestRegressor()\n",
        "    forest_model.fit(data_features_forest_train, data_target_forest_train)\n",
        "    forest_model.score(data_features_forest_test, data_target_forest_test)\n",
        "\n",
        "    #*BOOST REGRESSION MODEL\n",
        "    from sklearn.ensemble import GradientBoostingRegressor\n",
        "    boost_model = GradientBoostingRegressor()\n",
        "    boost_model.fit(data_features_boost_train, data_target_boost_train)\n",
        "    boost_model.score(data_features_boost_test, data_target_boost_test)\n",
        "\n",
        "    #*ALL MODEL REGRESSION PREDICTION\n",
        "    linear_model_prediction = linear_model.predict(data_big_features_train)\n",
        "    tree_model_prediction = tree_model.predict(data_big_features_train)\n",
        "    forest_model_prediction = forest_model.predict(data_big_features_train)\n",
        "    boost_model_prediction = boost_model.predict(data_big_features_train)\n",
        "\n",
        "    #*ENSEMBLE MODEL TRAINING DATA\n",
        "    ensemble_data = pd.DataFrame()\n",
        "    ensemble_data['linear'] = linear_model_prediction\n",
        "    ensemble_data['tree'] = tree_model_prediction\n",
        "    ensemble_data['forest'] = forest_model_prediction\n",
        "    ensemble_data['boost'] = boost_model_prediction\n",
        "    ensemble_target = data_big_target_train\n",
        "\n",
        "    #*ENSEMBLE MODEL TRAINING\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    ensemble_features_tensor = torch.tensor(ensemble_data.values, dtype=torch.float32)\n",
        "    ensemble_target_tensor = torch.tensor(ensemble_target.values, dtype=torch.float32)\n",
        "    \n",
        "    class Neural_Regressor(nn.Module):\n",
        "\n",
        "        def __init__(self, input_size, layers, p=0.4):\n",
        "            super().__init__()\n",
        "\n",
        "            all_layers = []\n",
        "\n",
        "            for i in layers:\n",
        "                all_layers.append(nn.Linear(input_size, i))\n",
        "                all_layers.append(nn.ReLU(inplace=True))\n",
        "                all_layers.append(nn.Dropout(p))\n",
        "                input_size = i\n",
        "\n",
        "            all_layers.append(nn.Linear(layers[-1], 1))\n",
        "\n",
        "            self.layers = nn.Sequential(*all_layers)\n",
        "\n",
        "        def forward(self, x_tensor):\n",
        "            logits = self.layers(x_tensor)\n",
        "            return logits\n",
        "        \n",
        "    class CustomDataset(TensorDataset):\n",
        "        def __init__(self, x_tensor, y_tensor):\n",
        "            super().__init__(x_tensor, y_tensor)\n",
        "            self.x_tensor = x_tensor\n",
        "            self.y_tensor = y_tensor\n",
        "            \n",
        "        def __getitem__(self, index):\n",
        "            return self.x_tensor[index], self.y_tensor[index]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.y_tensor)\n",
        "                \n",
        "    #*HYPERPARAMETER 2\n",
        "    lr = 0.05\n",
        "    epochs = 20\n",
        "    hidden_layers = [6,3,2]\n",
        "    dropout_p = 0.1\n",
        "    batch_size = 20\n",
        "    ensemble_model = Neural_Regressor(4, hidden_layers, p=dropout_p)\n",
        "    print(ensemble_model)\n",
        "    train_dataset = CustomDataset(ensemble_features_tensor, ensemble_target_tensor)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    loss_fn = nn.L1Loss()\n",
        "    optimizer = optim.Adagrad(ensemble_model.parameters(), lr=lr)\n",
        "    \n",
        "    def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "        size = len(dataloader.dataset)\n",
        "        model.train()\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            pred = model(X)\n",
        "            pred = pred.squeeze()\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if batch % 10 == 0:\n",
        "                loss, current = loss.item(), (batch + 1) * len(X)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "                \n",
        "    def test_loop(dataloader, model, loss_fn):\n",
        "        model.eval()\n",
        "        num_batches = len(dataloader)\n",
        "        test_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in dataloader:\n",
        "                pred = model(X)\n",
        "                pred = pred.squeeze()\n",
        "                test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "        test_loss /= num_batches\n",
        "        print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
        "    \n",
        "    #*ENSEMBLE MODEL TESTING DATA\n",
        "    linear_test_prediction = linear_model.predict(data_big_features_test)\n",
        "    tree_test_prediction = tree_model.predict(data_big_features_test)\n",
        "    forest_test_prediction = forest_model.predict(data_big_features_test)\n",
        "    boost_test_prediction = boost_model.predict(data_big_features_test)\n",
        "    \n",
        "    #*ENSEMBLE MODEL TESTING\n",
        "    ensemble_test_data = pd.DataFrame()\n",
        "    ensemble_test_data['linear'] = linear_test_prediction\n",
        "    ensemble_test_data['tree'] = tree_test_prediction\n",
        "    ensemble_test_data['forest'] = forest_test_prediction\n",
        "    ensemble_test_data['boost'] = boost_test_prediction\n",
        "    ensemble_test_target = data_big_target_test\n",
        "    \n",
        "    ensemble_test_features_tensor = torch.tensor(ensemble_test_data.values, dtype=torch.float32)\n",
        "    ensemble_test_target_tensor = torch.tensor(ensemble_test_target.values, dtype=torch.float32)\n",
        "    \n",
        "    test_dataset = CustomDataset(ensemble_test_features_tensor, ensemble_test_target_tensor)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        train_loop(train_loader, ensemble_model, loss_fn, optimizer)\n",
        "        test_loop(test_loader, ensemble_model, loss_fn)\n",
        "    print(\"Done!\")   \n",
        "\n",
        "\n",
        "model_training()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
